import os
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import requests
from supabase import create_client, Client
import supabase
from supabase_auth import datetime


# Load variables and create the CLIENT object
load_dotenv()
url = os.environ.get("SUPABASE_URL")
key = os.environ.get("SUPABASE_ANON_KEY")

# We will call our client 'supabase_db' to avoid confusion with the library name
supabase_db: Client = create_client(url, key)


def get_texts(td): 
    return [p.get_text(strip=True) for p in td.find_all('p')]



def scrape_fights_for_event(event_name, event_url): 
    try: 
        response = requests.get(event_url) 
        soup = BeautifulSoup(response.text, 'html.parser') 
        table_body = soup.find('tbody', class_='b-fight-details__table-body') 
        rows = table_body.find_all('tr', class_='b-fight-details__table-row') 
    except Exception as e: 
        print(f"‚ùå Error loading {event_url}: {e}") 
        return []
    
    fights = [] 
    
    for row in rows: 
        cols = row.find_all('td') 
        if len(cols) < 10: 
            continue

        result_flags = [r.lower() for r in get_texts(cols[0])] 
        
        if result_flags == ['win']: 
            result = 'win' 
        elif all(r == 'draw' for r in result_flags): 
            result = 'draw' 
        elif all(r == 'nc' for r in result_flags): 
            result = 'no contest' 
        else: result = 'unknown'

        fighters = get_texts(cols[1]) 
        fighter1 = fighters[0] if len(fighters) > 0 else '' 
        fighter2 = fighters[1] if len(fighters) > 1 else ''
        winner = fighter1 if result == 'win' else None

        fight_link = cols[0].find('a') 
        fight_url = fight_link['href'] if fight_link else None

        fight_name = f"{fighter1} vs {fighter2}" 
        
        fights.append({ 'event_name': event_name, 'fight': fight_name, 'winner': winner, 'fight_url': fight_url }) 

    return fights



def time_to_seconds(time_str):
    """Converts a MM:SS string into total seconds."""
    if not time_str or ":" not in time_str:
        return 0
    minutes, seconds = map(int, time_str.strip().split(":"))
    return minutes * 60 + seconds


def extract_int(cell):
    """Safely extracts an integer from a string cell."""
    try: 
        return int(cell.strip())
    except: 
        return 0


def extract_pct(cell):
    """Safely extracts a percentage float from a string."""
    return float(cell.strip('%')) if '%' in cell else None


def safe_split(text):
    """Splits 'X of Y' strings into two integers."""
    try:
        landed, attempted = text.split(' of ')
        return int(landed), int(attempted)
    except:
        return 0, 0


def load_fight_html_with_round_table(fight_url):
    """Fetches HTML content for a specific fight URL."""
    try:
        res = requests.get(fight_url)
        if res.status_code != 200:
            print(f"‚ùå Failed to fetch {fight_url}")
            return None
        return res.text
    except Exception as e:
        print(f"‚ùå Exception fetching fight: {e}")
        return None


def fight_already_in_db(fight_name, event_name, expected_rounds, fighter_names):
    """Checks if round stats for a specific fight already exist in Supabase."""
    result = supabase_db.table("round_fight_stats") \
        .select("round, fighter_name") \
        .eq("fight_name", fight_name) \
        .eq("event_name", event_name) \
        .execute()

    rows = result.data
    if not rows:
        return False

    seen = {(row['round'], row['fighter_name']) for row in rows}
    expected = {
        (round_num, fighter) 
        for round_num in range(1, expected_rounds + 1) 
        for fighter in fighter_names
    }

    return expected.issubset(seen)


def parse_base_stats_table(table, event_name, fight_name):
    """Parses the main Totals table for round-by-round data."""
    tbody = table.find('tbody')
    rows = tbody.find_all(['thead', 'tr'], recursive=False)

    stats = []
    round_number = 0
    i = 0

    while i < len(rows):
        row = rows[i]
        if row.name == 'thead' and 'Round' in row.text:
            round_number += 1
            i += 1
            if i >= len(rows): break

            tr = rows[i]
            tds = tr.find_all('td')
            if len(tds) != 10:
                i += 1
                continue

            try:
                f1 = [td.find_all('p')[0].text.strip() for td in tds]
                f2 = [td.find_all('p')[1].text.strip() for td in tds]
            except:
                i += 1
                continue

            for fighter in [f1, f2]:
                stats.append({
                    "event_name": event_name,
                    "fight_name": fight_name,
                    "fighter_name": fighter[0],
                    "round": round_number,
                    "kd": int(fighter[1]),
                    "sig_strikes_landed": int(fighter[2].split(' of ')[0]),
                    "sig_strikes_attempted": int(fighter[2].split(' of ')[1]),
                    "sig_strike_pct": float(fighter[3].replace('%','')) if '%' in fighter[3] else None,
                    "total_strikes_landed": int(fighter[4].split(' of ')[0]),
                    "total_strikes_attempted": int(fighter[4].split(' of ')[1]),
                    "takedowns_landed": int(fighter[5].split(' of ')[0]),
                    "takedowns_attempted": int(fighter[5].split(' of ')[1]),
                    "takedown_pct": float(fighter[6].replace('%','')) if '%' in fighter[6] else None,
                    "sub_attempts": int(fighter[7]),
                    "reversals": int(fighter[8]),
                    "control_time": fighter[9],
                    "control_time_sec": time_to_seconds(fighter[9])
                })
        i += 1
    return stats


def parse_zone_stats_table(table, event_name, fight_name):
    """Parses the Significant Strikes table for head/body/leg/distance data."""
    tbody = table.find('tbody')
    rows = tbody.find_all(['thead', 'tr'], recursive=False)

    stats = []
    round_number = 0
    i = 0

    while i < len(rows):
        row = rows[i]
        if row.name == 'thead' and 'Round' in row.text:
            round_number += 1
            i += 1
            if i >= len(rows): break

            tr = rows[i]
            tds = tr.find_all('td')
            if len(tds) < 9:
                print(f"‚ö†Ô∏è Skipping round {round_number}: unexpected column count ({len(tds)})")
                i += 1
                continue

            fighter1 = {"fighter_name": tds[0].find_all("p")[0].text.strip(), "round": round_number}
            fighter2 = {"fighter_name": tds[0].find_all("p")[1].text.strip(), "round": round_number}

            keys = [
                "sig_strikes_head", "sig_strikes_body", "sig_strikes_leg",
                "sig_strikes_distance", "sig_strikes_clinch", "sig_strikes_ground"
            ]

            for offset, key in enumerate(keys, start=3):
                f1_val = tds[offset].find_all("p")[0].text.strip()
                f2_val = tds[offset].find_all("p")[1].text.strip()
                f1_l, f1_a = safe_split(f1_val)
                f2_l, f2_a = safe_split(f2_val)

                fighter1[f"{key}_landed"] = f1_l
                fighter1[f"{key}_attempted"] = f1_a
                fighter2[f"{key}_landed"] = f2_l
                fighter2[f"{key}_attempted"] = f2_a

            stats.extend([fighter1, fighter2])
        i += 1
    return stats


def merge_round_and_zone_stats(main_stats, zone_stats):
    """Combines the two dictionaries based on fighter name and round number."""
    zone_lookup = {(z["fighter_name"], z["round"]): z for z in zone_stats}
    return [{**main, **zone_lookup.get((main["fighter_name"], main["round"]), {})} for main in main_stats]


def get_processed_fights():
    """Retrieves all fight/event combinations already stored in Supabase."""
    result = supabase_db.table("round_fight_stats").select("fight_name", "event_name").execute()
    return {(row["fight_name"], row["event_name"]) for row in result.data}


def get_latest_event():
    """Fetch the latest event record from the ufc_events table."""
    response = supabase_db.table("ufc_events").select("event_name, event_date").order("event_date", desc=True).limit(1).execute()
    return response.data[0] if response.data else None


def get_fights_for_event(event_name):
    """Retrieves all fight links for a specific event from the fights table."""
    response = supabase_db.table("fights").select("fight_url, fight, event_name").eq("event_name", event_name).execute()
    return response.data


def get_inserted_stats_for_event(event_name):
    """Checks which specific fighter rounds are already inserted for an event."""
    response = supabase_db.table("round_fight_stats").select("fight_name, round, fighter_name").eq("event_name", event_name).execute()
    return {(row['fight_name'], row['round'], row['fighter_name']) for row in response.data}


def fight_fully_inserted(fight_name, expected_stats, inserted_stats_set):
    """Verifies if every expected round/fighter row is present in the database."""
    for stat in expected_stats:
        key = (fight_name, stat["round"], stat["fighter_name"])
        if key not in inserted_stats_set:
            return False
    return True


def scrape_latest_event():
    """Orchestrates the scraping of the most recent event's round-by-round stats."""
    latest_event = get_latest_event()
    if not latest_event:
        print("‚ùå No event found.")
        return

    event_name = latest_event["event_name"]
    print(f"üîç Checking latest event: {event_name}")

    fights = get_fights_for_event(event_name)
    inserted = get_inserted_stats_for_event(event_name)

    new_rows = []

    for fight in fights:
        fight_name = fight["fight"]
        fight_url = fight["fight_url"]
        print(f"üîç Scraping fight: {fight_name}")

        html = load_fight_html_with_round_table(fight_url)
        if not html:
            continue

        soup = BeautifulSoup(html, 'html.parser')
        tables = soup.find_all('table', class_='b-fight-details__table js-fight-table')

        if len(tables) < 2:
            print("‚ö†Ô∏è Skipping ‚Äî expected both round and zone tables.")
            continue

        main_stats = parse_base_stats_table(tables[0], event_name, fight_name)
        zone_stats = parse_zone_stats_table(tables[1], event_name, fight_name)
        merged_stats = merge_round_and_zone_stats(main_stats, zone_stats)

        if fight_fully_inserted(fight_name, merged_stats, inserted):
            print(f"‚è≠Ô∏è Already complete: {fight_name}")
            continue

        new_rows.extend(merged_stats)
        print(f"‚úÖ Collected {len(merged_stats)} rows from {fight_name}")

    if new_rows:
        supabase_db.table("round_fight_stats").insert(new_rows).execute()
        print(f"üü¢ Inserted {len(new_rows)} rows from {event_name}.")
    else:
        print("‚úÖ No new rows to insert ‚Äî everything is up to date.")


def get_latest_events(limit=5):
    response = supabase_db.table("ufc_events") \
        .select("event_name, event_date") \
        .order("event_date", desc=True) \
        .limit(limit) \
        .execute()
    return response.data if response.data else []


# def weekly_scrape_check(min_events=2):
    print(f"üîÅ Weekly check: Will scan at least {min_events} recent events.")
    events = get_latest_events(limit=10)
    events_to_check = []

    for i, event in enumerate(events):
        event_name = event["event_name"]
        fights = get_fights_for_event(event_name)
        inserted = get_inserted_stats_for_event(event_name)

        all_complete = True
        for fight in fights:
            fight_name = fight["fight"]
            fight_url = fight["fight_url"]

            html = load_fight_html_with_round_table(fight_url)
            if not html:
                continue

            soup = BeautifulSoup(html, 'html.parser')
            tables = soup.find_all('table', class_='b-fight-details__table js-fight-table')
            if len(tables) < 2:
                continue

            main_stats = parse_base_stats_table(tables[0], event_name, fight_name)
            zone_stats = parse_zone_stats_table(tables[1], event_name, fight_name)
            merged_stats = merge_round_and_zone_stats(main_stats, zone_stats)

            if not fight_fully_inserted(fight_name, merged_stats, inserted):
                all_complete = False
                break

        events_to_check.append((event_name, not all_complete))

        if i + 1 >= min_events and all_complete:
            break

    total_inserted = 0
    for event_name, needs_scrape in events_to_check:
        if not needs_scrape:
            print(f"‚úÖ {event_name} already fully scraped.")
            continue

        print(f"üîç Scraping missing stats for: {event_name}")
        fights = get_fights_for_event(event_name)
        inserted = get_inserted_stats_for_event(event_name)
        event_rows = []

        for fight in fights:
            fight_name = fight["fight"]
            fight_url = fight["fight_url"]

            html = load_fight_html_with_round_table(fight_url)
            if not html:
                continue

            soup = BeautifulSoup(html, 'html.parser')
            tables = soup.find_all('table', class_='b-fight-details__table js-fight-table')
            if len(tables) < 2:
                continue

            main_stats = parse_base_stats_table(tables[0], event_name, fight_name)
            zone_stats = parse_zone_stats_table(tables[1], event_name, fight_name)
            merged_stats = merge_round_and_zone_stats(main_stats, zone_stats)

            if fight_fully_inserted(fight_name, merged_stats, inserted):
                print(f"‚è≠Ô∏è Already complete: {fight_name}")
                continue

            event_rows.extend(merged_stats)

        if event_rows:
            supabase_db.table("round_fight_stats").insert(event_rows).execute()
            print(f"üü¢ Inserted {len(event_rows)} rows for {event_name}.")
            total_inserted += len(event_rows)

    print(f"üèÅ Weekly scrape complete. Total new rows: {total_inserted}.")





def sync_incomplete_fights():
    """Queries the SQL View for any missing or partial fight data and fills the gaps."""
    print("üì° Querying database view for missing round data...")
    # Use the View we created!
    response = supabase_db.table("fight_scraping_status") \
        .select("fight_name, event_name, fight_url, fight_status") \
        .filter("fight_status", "in", '("‚ùå MISSING", "‚ö†Ô∏è PARTIAL")') \
        .execute()

    missing_tasks = response.data

    if not missing_tasks:
        print("‚úÖ Everything is up to date! No gaps found in the rounds data.")
        return

    print(f"üîç Found {len(missing_tasks)} fights needing data. Starting sync...")

    for task in missing_tasks:
        f_name = task['fight_name']
        e_name = task['event_name']
        f_url = task['fight_url']
        
        print(f"üîÑ Syncing: {f_name} ({e_name})")

        html = load_fight_html_with_round_table(f_url)
        if not html: continue

        soup = BeautifulSoup(html, 'html.parser')
        tables = soup.find_all('table', class_='b-fight-details__table js-fight-table')

        if len(tables) < 2:
            print(f"‚ö†Ô∏è Skipping {f_name} ‚Äî Round tables not found.")
            continue

        # Use your existing parsing logic
        main_stats = parse_base_stats_table(tables[0], e_name, f_name)
        zone_stats = parse_zone_stats_table(tables[1], e_name, f_name)
        merged_stats = merge_round_and_zone_stats(main_stats, zone_stats)

        if merged_stats:
            try:
                # IMPORTANT: Get what's already there to avoid duplicate key errors
                inserted = get_inserted_stats_for_event(e_name)
                
                rows_to_add = [
                    row for row in merged_stats 
                    if (row['fight_name'], row['round'], row['fighter_name']) not in inserted
                ]

                if rows_to_add:
                    supabase_db.table("round_fight_stats").insert(rows_to_add).execute()
                    print(f"üü¢ Added {len(rows_to_add)} rows for {f_name}.")
            except Exception as e:
                print(f"‚ùå Error inserting {f_name}: {e}")


# --- Run the weekly job ---
#weekly_scrape_check()


# --- EXECUTION ---
if __name__ == "__main__":
    # 1. First, you could run your event/fight scraper here if you have one
    # 2. Finally, run the sync to catch everything missing
    sync_incomplete_fights()