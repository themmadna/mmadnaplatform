import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import time
import logging
import argparse
from datetime import datetime

# --- CONFIGURATION ---
DEFAULT_START_YEAR = 2010
CURRENT_YEAR = datetime.now().year 
STOP_THRESHOLD = 3 
# ---------------------

logging.basicConfig(filename='scrape_errors.log', level=logging.ERROR, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

def fetch_page(url):
    print(f"Fetching: {url}")
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
        response.raise_for_status()
        time.sleep(1.5)
        return response.text
    except Exception as e:
        logging.error(f"Failed to fetch {url}: {e}")
        return None

def extract_fight_data(html_content, url):
    if not html_content: return None
    soup = BeautifulSoup(html_content, "html.parser")
    
    event_block = soup.find("td", class_="decision-top2")
    event_lines = [line.strip() for line in event_block.text.splitlines() if line.strip()] if event_block else []
    event = event_lines[0] if event_lines else 'N/A'
    date = event_lines[1] if len(event_lines) > 1 else 'N/A'
    
    referee_block = soup.find("td", class_="decision-bottom2")
    referee = referee_block.get_text(strip=True).replace('REFEREE:', '').strip() if referee_block else 'N/A'
    
    # Use URL to get fighter names but strip any weird trailing characters
    fight_name_raw = url.split('/')[-1].replace('-', ' ').strip()
    try:
        f1_url, f2_url = [f.strip() for f in fight_name_raw.split(' vs ')]
    except ValueError: 
        f1_url = f2_url = 'Unknown'
    
    data = []
    judge_tables = soup.find_all("table", style="border-spacing: 1px; width: 100%")
    for table in judge_tables:
        try:
            # .strip() handles the \n issues
            judge = table.find("a").get_text(strip=True).replace("\xa0", " ").strip()
        except AttributeError: continue
        
        for round_row in table.find_all("tr", class_="decision"):
            cols = round_row.find_all("td", class_="list")
            if len(cols) < 3 or not cols[1].text.strip() or cols[1].text.strip() == "-": continue
            
            # Match your CSV column order exactly
            # event,bout,date,fighter,judge,round,score,referee
            bout_name = f"{f1_url} vs. {f2_url}"
            
            common_fields = {
                'event': event.strip(),
                'bout': bout_name,
                'date': date.strip(),
                'judge': judge,
                'round': cols[0].text.strip(),
                'referee': referee
            }
            
            # Row for Fighter 1
            data.append({**common_fields, 'fighter': f1_url, 'score': cols[1].text.strip()})
            # Row for Fighter 2
            data.append({**common_fields, 'fighter': f2_url, 'score': cols[2].text.strip()})
            
    return {'data': data} if data else None

def scrapeDataFunction(start_year, end_year):
    url = "http://mmadecisions.com/decisions-by-event/"
    base_url = "http://mmadecisions.com/"
    output_file = 'ufc_fight_scores.csv'
    
    already_scraped = set()
    if os.path.exists(output_file):
        try:
            existing = pd.read_csv(output_file)
            # STRIP EVERYTHING coming from the CSV to ensure the match works
            existing['bout'] = existing['bout'].astype(str).str.strip()
            existing['date'] = existing['date'].astype(str).str.strip()
            
            for _, row in existing[['bout', 'date']].drop_duplicates().iterrows():
                already_scraped.add(f"{row['bout']}|{row['date']}")
            print(f"Loaded {len(already_scraped)} clean bouts from file.")
        except Exception as e: 
            print(f"Error reading CSV: {e}")

    main_html = fetch_page(url)
    if not main_html: return
    soup = BeautifulSoup(main_html, 'html.parser')
    year_cells = [y.text for y in soup.find('table', width="100%").find_all('td') if y.text.isdigit()]
    years_to_process = sorted([y for y in year_cells if start_year <= int(y) <= end_year], reverse=True)

    events_skipped_in_a_row = 0

    for y in years_to_process:
        print(f"\n--- Checking Year: {y} ---")
        year_html = fetch_page(f"{url}{y}/")
        if not year_html: continue
        year_soup = BeautifulSoup(year_html, 'html.parser')
        event_links = [a.get('href') for a in year_soup.find_all('a') if 'UFC' in a.text]
        
        for e_link in event_links:
            event_html = fetch_page(base_url + e_link)
            if not event_html: continue
            bout_soup = BeautifulSoup(event_html, 'html.parser')
            bouts = [a.get('href') for a in bout_soup.find_all('a') if 'decision/' in a.get('href', '')]
            
            new_fights_in_this_event = 0
            for b_link in bouts:
                # Build the lookup key before scraping the full page
                raw_slug = b_link.split('/')[-1].replace('-', ' ').strip()
                try:
                    f1_url, f2_url = [f.strip() for f in raw_slug.split(' vs ')]
                    clean_id_bout = f"{f1_url} vs. {f2_url}"
                except: continue

                # Look for this bout name in our 'already_scraped' set
                if any(clean_id_bout in key for key in already_scraped):
                    continue

                res = extract_fight_data(fetch_page(base_url + b_link.strip()), base_url + b_link)
                if res:
                    actual_id = f"{res['data'][0]['bout']}|{res['data'][0]['date']}"
                    if actual_id in already_scraped:
                        continue

                    df = pd.DataFrame(res['data'])
                    # Reorder columns to match your desired format
                    df = df[['event', 'bout', 'date', 'fighter', 'judge', 'round', 'score', 'referee']]
                    df.to_csv(output_file, mode='a', index=False, header=not os.path.isfile(output_file))
                    print(f"   [SAVED] {res['data'][0]['bout']}")
                    already_scraped.add(actual_id)
                    new_fights_in_this_event += 1

            if new_fights_in_this_event == 0 and len(bouts) > 0:
                events_skipped_in_a_row += 1
                print(f" > No new data in event. (Consecutive: {events_skipped_in_a_row})")
            elif new_fights_in_this_event > 0:
                events_skipped_in_a_row = 0 

            if events_skipped_in_a_row >= STOP_THRESHOLD:
                print(f"\nReached {STOP_THRESHOLD} consecutive existing events. Stopping.")
                return

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", type=int, default=DEFAULT_START_YEAR)
    parser.add_argument("--end", type=int, default=CURRENT_YEAR)
    args = parser.parse_args()

    if input(f"Start incremental scrape? (yes/no): ").lower() == 'yes':
        scrapeDataFunction(args.start, args.end)
