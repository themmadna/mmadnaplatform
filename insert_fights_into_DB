import os
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import requests
from supabase import create_client, Client
import supabase
from supabase_auth import datetime


# Load variables and create the CLIENT object
load_dotenv()
url = os.environ.get("SUPABASE_URL")
key = os.environ.get("SUPABASE_ANON_KEY")

# We will call our client 'supabase_db' to avoid confusion with the library name
supabase_db: Client = create_client(url, key)


def get_all_event_links(): 
    url = "http://ufcstats.com/statistics/events/completed?page=all" 
    response = requests.get(url) 
    soup = BeautifulSoup(response.text, 'html.parser') 
    table_rows = soup.select('tr.b-statistics__table-row')

    events = [] 
    
    for row in table_rows: 
        if row.find('img'): 
            continue

        cols = row.find_all('td') 
        if len(cols) != 2: 
            continue

        event_tag = cols[0].find('a') 
        event_date_tag = cols[0].find('span', class_='b-statistics__date')

        if not event_tag or not event_date_tag: 
            continue

        event_name = event_tag.text.strip() 
        event_url = event_tag['href'].strip() 
        event_date = event_date_tag.text.strip()


        try: 
            event_datetime = datetime.strptime(event_date, "%B %d, %Y") 
            if event_datetime > datetime.today(): 
                continue 
        except: 
            continue 

        events.append({ 'event_name': event_name, 'event_url': event_url }) 

    return events


def get_texts(td): 
    return [p.get_text(strip=True) for p in td.find_all('p')]


def scrape_fights_for_event(event_name, event_url): 
    try: 
        response = requests.get(event_url) 
        soup = BeautifulSoup(response.text, 'html.parser') 
        table_body = soup.find('tbody', class_='b-fight-details__table-body') 
        rows = table_body.find_all('tr', class_='b-fight-details__table-row') 
    except Exception as e: 
        print(f"âŒ Error loading {event_url}: {e}") 
        return []
    
    fights = [] 
    
    for row in rows: 
        cols = row.find_all('td') 
        if len(cols) < 10: 
            continue

        result_flags = [r.lower() for r in get_texts(cols[0])] 
        
        if result_flags == ['win']: 
            result = 'win' 
        elif all(r == 'draw' for r in result_flags): 
            result = 'draw' 
        elif all(r == 'nc' for r in result_flags): 
            result = 'no contest' 
        else: result = 'unknown'

        fighters = get_texts(cols[1]) 
        fighter1 = fighters[0] if len(fighters) > 0 else '' 
        fighter2 = fighters[1] if len(fighters) > 1 else ''
        winner = fighter1 if result == 'win' else None

        fight_link = cols[0].find('a') 
        fight_url = fight_link['href'] if fight_link else None

        fight_name = f"{fighter1} vs {fighter2}" 
        
        fights.append({ 'event_name': event_name, 'fight': fight_name, 'winner': winner, 'fight_url': fight_url }) 

    return fights


def insert_fights_without_duplicates(fights): 
    for fight in fights: 
        # Check if fight_url already exists 
        existing = supabase_db.table('fights').select('id').eq('fight_url', fight['fight_url']).execute() 
        if existing.data: 
            print(f"âš ï¸ Skipping duplicate: {fight['fight']}") 
            continue 
        
        # Insert fight 
        try: 
            supabase_db.table('fights').insert(fight).execute() 
            print(f"âœ… Inserted: {fight['fight']}") 
        except Exception as e: 
            print(f"âŒ Failed to insert {fight['fight']}: {e}")


def scrape_and_insert_new_fights():
    events = get_all_event_links()
    print(f"ğŸ” Checking {len(events)} events for new fights...")

    for event in events:
        event_name = event['event_name']
        event_url = event['event_url']

        # Check if *any* fight already exists for this event (based on event_name)
        existing = supabase_db.table("fights").select("id").eq("event_name", event_name).execute()

        if existing.data:
            print(f"ğŸ›‘ Skipping event (already scraped): {event_name}")
            break  # Stop once we hit an old event (assumes newest-first order)

        print(f"ğŸ”„ Scraping fights for: {event_name}")
        fights = scrape_fights_for_event(event_name, event_url)

        if fights:
            insert_fights_without_duplicates(fights)
        else:
            print(f"âš ï¸ No fights found for {event_name}")

# Run
scrape_and_insert_new_fights()